<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2013-08-10-logistic_regression_in_lisp</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="mlisp.css" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <base href="https://kaygun.github.io/">
  <link rel="stylesheet" type="text/css" href="style.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="logistic-regression-in-lisp">Logistic Regression in lisp</h1>
<h2 id="description-of-the-problem">Description of the problem</h2>
<p>I was looking at the code I wrote for my <a
href="2013-07-17-a_gradient_descent_implementation_in_lisp.html">Gradient
Descent</a> post and there is room for improvement. I will refactor the
code, and add a new application: I will do <a
href="http://en.wikipedia.org/wiki/Logistic_regression">Logistic
Regression</a> using Gradient Descent.</p>
<h2 id="ordinary-least-square-regression-revisited">Ordinary least
square regression revisited</h2>
<p>Let me go over the setup: we have a finite dataset <span
class="math inline">\(x^{(1)},\ldots,x^{(n)}\)</span> of points coming
from a <span class="math inline">\(m\)</span>-dimensional vector space.
For each of these points we have a numerical response variable <span
class="math inline">\(y^{(i)}\)</span> for <span
class="math inline">\(i=1,\ldots,n\)</span>. We want to approximate
<span class="math inline">\(y^{(i)}\)</span> using <span
class="math inline">\(x^{(i)}\)</span> via a linear equation of the type
<span class="math display">\[
y^{(i)} = {\mathbf a}\cdot x^{(i)} + b \]</span> In the ordinary least
squares resgression we minimize the total error: <span
class="math display">\[ LSE({\mathbf
a},b) = \frac{1}{2n}\sum_{i=1}^n \|y^{(i)}-{\mathbf a}\cdot
x^{(i)}-b\|^2 \]</span> over the parameters <span
class="math inline">\({\mathbf a}\)</span> and <span
class="math inline">\(b\)</span>.</p>
<h2 id="logistic-regression">Logistic regression</h2>
<p>If the responses <span class="math inline">\(y^{(i)}\)</span> are
discrete, specifically if they take only two values (say 0 and 1), then
we can use another statistical model called <a
href="http://en.wikipedia.org/wiki/Logistic_regression">Logistic
Regression</a>. The biggest difference comes from the fact that this
problem is not a regression problem, rather a classification problem. We
must split the vector space (and therefore the dataset) into 2 halves so
that datapoints with <span class="math inline">\(y^{(i)}=0\)</span> and
<span class="math inline">\(y^{(i)}=1\)</span> lie on different
sides.</p>
<p>Notice that <span class="math display">\[ {\mathbf a}\cdot {\mathbf
x} + b = 0\]</span> is an equation of a hyper-plane and each side of the
hyper-plane is determined by the sign of the auxiliary function <span
class="math display">\[ f({\mathbf x}) = {\mathbf
a}\cdot {\mathbf x} + b \]</span> Larger the norm <span
class="math inline">\(\|f({\mathbf x})\|\)</span> the more confident one
has to be that <span class="math inline">\({\mathbf x}\)</span> belongs
to the correct side of the hyper-plane in question. At this stage, the
<a href="https://en.wikipedia.org/wiki/Logistic_function">Logistic
Function</a> <span class="math inline">\(\frac{1}{1+exp(-x)}\)</span>
comes to our rescue.</p>
<figure class="tmblr-full" data-orig-height="375" data-orig-width="500" data-orig-src>
<img src="../media/2013-08-10-logistic_regression_in_lisp_0.png" data-orig-height="375"
data-orig-width="500" data-orig-src="" alt="image" />
</figure>
<p>Let us say that the probability of a point <span
class="math inline">\({\mathbb z}\)</span> being on the positive side of
the hyper-plane <span class="math inline">\({\mathbf a}\cdot {\mathbf
x}+b=0\)</span> is <span class="math display">\[
\frac{1}{1+\exp(-{\mathbf a}\cdot {\mathbf z} - b)} \]</span> Using this
we can write a probability function <span
class="math inline">\(p({\mathbf z},y)\)</span> <span
class="math display">\[ p({\mathbf z},y) =
\left(\frac{1}{1+\exp(-{\mathbf a}\cdot
{\bf z} - b)}\right)^y \left(\frac{1}{1+\exp({\mathbf a}\cdot
{\bf z} + b)}\right)^{1-y} \]</span> which measures the probability of
<span class="math inline">\({\bf z}\)</span> being on the right side
where <span class="math inline">\(y\)</span> takes values 0 or 1. Then
the probability that all of the points in our dataset being on the right
side is <span class="math display">\[ p(D) = \prod_{i=1}^n
p(x^{(i)},y^{(i)}) \]</span> and therefore we would like to maximize
<span class="math inline">\(p(D)\)</span> depending on the unknown
parameters <span class="math inline">\({\mathbf a}\)</span> and <span
class="math inline">\(b\)</span>. Instead one can minimize the following
function <span class="math display">\[ LE({\mathbf a},b) =
-\frac{1}{n}\sum_{i=1}^n y^{(i)}\log(1+\exp(-{\mathbf a}\cdot
x^{(i)} - b)) + (1-y^{(i)})\log(1+\exp({\mathbf a}\cdot x^{(i)} +
b)) \]</span> which is just the natural logarithm of <span
class="math inline">\(p(D)\)</span>. This is the part I am going to
tackle with a new and slightly improved gradient descent.</p>
<h2 id="the-refactored-code">The refactored code</h2>
<p>Without further ado, here is the new refactored code.</p>
<p><br />
</p>
<pre><code>  (defun descent (fun x &amp;key (error 1.0d-4) 
                             (rate 1.0d-2) 
                             (debug nil)
                             (max-steps 1000))
    (labels
       ((make-step (y) 
            (loop for i from 0 below (length y) collect 
               (let ((ini (copy-seq y))
                     (ter (copy-seq y)))
                  (incf (aref ter i) error)
                  (decf (aref ini i) error)
                  (* (- (funcall fun ter) 
                        (funcall fun ini)) 
                     (/ rate error 2)))))
        (norm (y) (sqrt (reduce &#39;+ (map &#39;list (lambda (i) (* i i)) y)))))

      (do* ((y x (map &#39;vector &#39;- y step))
            (step (make-step y) (make-step y))
            (n 1 (1+ n)))
         ((or (&lt; (norm step) error) (&gt;= n max-steps)) (values y n))
         (if (and debug (zerop (mod n 100)))
            (format t &quot;~4D ~6,7F  (~{~4,2F~^, ~})~%&quot; n (norm step) (coerce y &#39;list))))))</code></pre>
<p><code></code></p>
<p>The two biggest differences are (1) I am using <code>labels</code>
instead of using <code>defun</code> for auxiliary code, and (2) the
derivative code is streamlined to be more efficient. In this form the
function <code>descent</code> is a standalone function. It doesnâ€™t need
anything else. You can cut-and-paste it as is and start using it.</p>
<h2 id="least-square-regression-re-revisited">Least square regression
re-revisited</h2>
<p>I will re-implement the least square regression again to test the
code.</p>
<pre><code>(defun least-square-err (data)
   (lambda (x)
       (* (/ 1.0d0 (length data))
          (loop for point in data sum
                (let* ((N (1- (length point)))
                       (z (copy-seq point))
                       (y (aref point N)))
                  (setf (aref z N) 1)
                  (expt (- y (reduce &#39;+ (map &#39;vector &#39;* z x))) 2))))))

(defparameter my-data
   (let ((a (- (random 4.0) 2.0))
         (b (- (random 2.0) 1.0)))
      (mapcar (lambda (x) (coerce (list x (+ b (* a x) (- (random 0.8) 0.4))) &#39;vector))
              (loop for i from 1 to 30 collect (random 2.0)))))

(defparameter my-err (least-square-err my-data))
(defparameter theta (descent my-err #(1.0 -1.2) 
                             :rate  9.0d-2
                             :error 1.0d-4
                             :max-steps 500 
                             :debug nil))</code></pre>
<figure class="tmblr-full" data-orig-height="375" data-orig-width="500" data-orig-src>
<img src="../media/2013-08-10-logistic_regression_in_lisp_1.png" data-orig-height="375"
data-orig-width="500" data-orig-src="" alt="image" />
</figure>
<h2 id="logistic-regression-1">Logistic regression</h2>
<p>Now, comes the implementation of the function we would like to
minimize</p>
<pre><code>(defun logistic-err (data)
   (lambda (x) 
       (* (/ -1.0d0 (length data))
          (loop for point in data sum 
             (let* ((y (copy-seq point))
                    (N (1- (length y)))
                    (b (aref y N))
                    res u)
                 (setf (aref y N) 1)
                 (setf res (reduce &#39;+ (map &#39;vector &#39;* x y)))
                 (setf u (/ 1.0d0 (+ 1.0d0 (exp res))))
                 (+ (* (log u) (- 1.0d0 b)) 
                    (* (log (- 1.0d0 u)) b)))))))</code></pre>
<p>As before, this function takes a dataset and then returns a function
that <code>descent</code> will try to minimize. Let me create an example
dataset to work on:</p>
<pre><code>(defparameter my-data 
   (labels 
      ((range (n) (loop for i from 0 to n collect i))
       (here (n b) 
            (map &#39;vector (lambda (x) (if (&lt; x n) (random 1.0) b)) (range n)))
       (there (a b)
            (map &#39;vector &#39;+ (append a (list 0)) (here (length a) b))))
     (union (loop repeat 12 collect (there &#39;(1.3 -0.6) 0))
            (loop repeat 10 collect (there &#39;(1.1  0.0) 1)))))</code></pre>
<p>First we create our function which we would like to minimize, and
then we feed it to <code>descent</code>:</p>
<pre><code>(defparameter my-err (logistic-err my-data))
(defparameter theta (descent my-err #(0.0 0.0 0.0) 
                                   :max-steps 2000 
                                   :error 1.0d-5
                                   :rate  9.0d-2
                                   :debug nil))</code></pre>
<p>Let us see the fitted data:</p>
<figure class="tmblr-full" data-orig-height="375" data-orig-width="500" data-orig-src>
<img src="../media/2013-08-10-logistic_regression_in_lisp_2.png" data-orig-height="375"
data-orig-width="500" data-orig-src="" alt="image" />
</figure>
<div id="footer">
<p><span id="timestamp">August 10th, 2013 2:16am</span></p>
</div>
</body>
</html>
