<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2013-10-15-self_organizing_maps</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="style.css" />
  <html>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <base href="https://kaygun.github.io/">
  <link rel="stylesheet" type="text/css" href="style.css">


  <head>
  <title>The Kitchen Sink and Other Oddities</title>
  </head>

  <body>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ]
    }
  });
  </script>

  <div class="header">
  <h1><a href="https://kaygun.github.io">The Kitchen Sink and Other Oddities</a></h1>
  <p><a href="https://web.itu.edu.tr/kaygun">Atabey Kaygun</a></p>
  </div>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="self-organizing-maps">Self Organizing Maps</h1>
<h2 id="description-of-the-problem">Description of the problem</h2>
<p><a href="http://www.scholarpedia.org/article/Kohonen_network">Self
Organizing Maps</a> (also known as Kohonen Networks) is a class of
unsupervised classification algorithms that descend from <a
href="http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html">Neural
Networks</a>. One distinguishing feature of SOM algorithms is that they
can be used to transform a high dimensional dataset into a visual form
which is usually a discrete lattice of small dimension.</p>
<p>Today, I will implement a SOM algorithm in lisp. I will use <a
href="http://en.wikipedia.org/wiki/Graph_(mathematics)#Undirected_graph">undirected
graphs</a> to encode topologies of which discrete lattices are specific
examples.</p>
<h2 id="topology">Topology</h2>
<p>As with any classification problem, we have a dataset <span
class="math inline"><em>D</em> = {<em>x</em><sup>(1)</sup>, …, <em>x</em><sup>(<em>m</em>)</sup>}</span>
where this time each <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is a vector
of dimension <span class="math inline"><em>n</em></span>. We would like
to develop a classification scheme <span
class="math display"><em>α</em>: <em>D</em> → {1, …, <em>p</em>}</span>
where <span class="math inline"><em>p</em></span> is determined
beforehand. But, instead of assuming that each site <span
class="math inline">1, …, <em>p</em></span> is an independent discrete
entity, we will assume that there is <em>topology</em>. That is some
sites are neighbors, ie. within <em>a vicinity</em> of each other. Since
we work with finite sets, the best description of such a neighborhood
system is by using a <a
href="http://en.wikipedia.org/wiki/Graph_(mathematics)#Undirected_graph">sub-basis</a>
for a topology. This data can also be described by undirected graph.</p>
<p>Here is such an example: assume we have 4 classes, conveniently
labelled by 0,1,2 and 3. Assume we are given the following neighborhood
system:</p>
<ul>
<li>The class 0 is neighbor with the class 1 only</li>
<li>The class 1 is neighbors with classes 0, 2 and 3</li>
<li>The class 2 is neighbors with classes 1 and 3</li>
<li>The class 3 is neighbors with classes 1 and 2</li>
</ul>
<p>We can represent this datum with an undirected graph as follows:</p>
<p><img src="../media/2013-10-15-self_organizing_maps_0.png" /></p>
<p>Our algorithm will utilize the underlying topology of the
classes.</p>
<h2 id="algorithm">Algorithm</h2>
<p>Our algorithm will use a undirected graph (indicating a topology on
the classes) in which every vertex (class) is labelled (called
<em>weights</em>) by a numerical vector of the same dimension as the
dimensions of the vectors in our dataset <span
class="math inline"><em>D</em></span>. We will perform iterations on
this graph. In each iteration, we take a small sample <span
class="math inline"><em>S</em></span> of points from <span
class="math inline"><em>D</em></span>. Then for each <span
class="math inline"><em>v</em> ∈ <em>S</em></span> we find the vertex
<span class="math inline"><em>α</em></span> whose weight <span
class="math inline"><em>ω</em>(<em>α</em>)</span> is closest to <span
class="math inline"><em>v</em></span>, measured by a fixed distance
function. Then we update the weight vector <span
class="math inline"><em>ω</em>(<em>α</em>)</span> using <span
class="math inline"><em>v</em></span>, and every <span
class="math inline"><em>ω</em>(<em>β</em>)</span> where <span
class="math inline"><em>β</em></span> runs over all neighbors of <span
class="math inline"><em>α</em></span>. In order to simplify our
algorithm, we will assume that every vertex is neighboring itself so
that when we iterate over the neighbors of a vertex <span
class="math inline"><em>α</em></span>, we also update the weight <span
class="math inline"><em>ω</em>(<em>α</em>)</span>. The update procedure
is described below in detail. We finish the iteration when weights
stabilize.</p>
<p>The update procedure is of the form <span
class="math display"><em>w</em> = <em>w</em> + <em>ϕ</em>(<em>w</em>,<em>v</em>)(<em>v</em>−<em>w</em>)</span>
where <span class="math inline"><em>w</em></span> is the weight of a
vertex and <span class="math inline"><em>v</em></span> is the data
point. we would like to have adjustment parameter <span
class="math inline"><em>ϕ</em>(<em>w</em>,<em>v</em>)</span> depend only
on the distance <span
class="math inline"><em>d</em>(<em>w</em>,<em>v</em>)</span>, and
preferably a fast decreasing one too.</p>
<p>We describe our algorithm in pseudo-code as follows:</p>
<pre><code>Algorithm SelfOrganizingMap
Input:
  D &lt;- dataset of vectors, each of dimension n
  Top &lt;- a topology of classes in the form of a
         neighborhood subbasis
  W &lt;- an initial set of weight vectors for each class
  dist &lt;- a distance measure for vectors of dimension n
  epochs &lt;- the number of iterations
Output:
  W &lt;- a final set of stabilized weight vectors
Begin
  Repeat epochs many times
    Let S be a randomly chosen small sample from D
    For each v in S do
        Let alpha be the vertex such that its weight 
            vector W(alpha) is the closest weight 
            vector to v among all vertices.
        For each neighboring vertex beta of alpha do
            Update beta as
               W(beta) &lt;- W(beta) + phi(d(v,beta))*(v - beta)
        End 
    End
  End
  Return(W)
End</code></pre>
<h2 id="an-implementation">An implementation</h2>
<p>I will implement the algorithm bottom up: let me start with finding
the closest weight vector to a given vector.</p>
<pre><code>(defun find-closest (v cloud dist)
  (let ((i 0)
        (res 1d10)
    (n (length cloud))
        temp)
    (dotimes (j n)
      (setf temp (funcall dist v (elt cloud j)))
      (if (&lt; temp res)
         (progn (setf res temp)
                (setf i j))))
    (cons i res)))</code></pre>
<p>Next, out update function.</p>
<pre><code>(defun update (d v w &amp;optional (rate 1.1))
    (map &#39;vector (lambda (i j) (+ i (* (/ rate (+ 1.0d0 d)) (- j i)))) v w))</code></pre>
<p>In the algorithm, I will need to take a small sample of vectors. I
will do this using their indices in a list/array:</p>
<pre><code>(defun sample (n upper-bound)
  (loop repeat n collect (random upper-bound)))</code></pre>
<p>Finally, here is our implementation:</p>
<pre><code>(defun train-som (data sample-size top weights epochs dist &amp;optional (rate 1.1))
     (let* ((m (length data))
             (k (truncate (* m sample-size)))
             (copy-w (copy-list weights)))
        (dotimes (i epochs)
          (let* ((S (mapcar (lambda (x) (elt data x)) (sample k m))))
            (dolist (v S)
              (let* ((w (find-closest v copy-w dist))
                         (neigh (cdr (elt top (car w)))))
                (dolist (x neigh)
                  (setf (elt copy-w x) 
                            (update (cdr w) (elt copy-w x) v rate)))))))
        copy-w))</code></pre>
<p>The parameters are</p>
<ul>
<li><code>data</code> which is going to be a list of vectors,</li>
<li><code>top</code> the topology given as a list of lists of
neighboring vertices for each class,</li>
<li><code>weights</code> an initial weight system (usually randomly
assigned),</li>
<li><code>epochs</code> the number of iterations</li>
<li><code>dist</code> the distance function</li>
<li><code>rate</code> the learning rate</li>
</ul>
<h2 id="an-example">An example</h2>
<p>For this example, I will use the <a
href="http://yann.lecun.com/exdb/mnist/">MNIST dataset of hand-written
digits</a> from Yann LeCun’s site. First I will need to load up the
data. The format of the data files is explained in <a
href="http://yann.lecun.com/exdb/mnist/">LeCun’s website</a>. I have two
datasets: one for the real classification of the data points (which
digit it is) and the other is a 29x29 array of pixel values ranging from
0 to 255.</p>
<pre><code>(defun get-term (in) 
  (let ((temp 0))
    (loop for i from 1 to 4 do
       (setf (ldb (byte 8 (* 8 (- 4 i))) temp) (read-byte in)))
    temp))

(defun read-labels (file-name)
  (with-open-file (in file-name
                  :direction :input
                  :element-type &#39;(unsigned-byte 8))
      (let ((magic (get-term in))
            (number (get-term in)))
         (loop repeat number collect 
            (ldb (byte 8 0) (read-byte in))))))

(defun read-images (file-name)
  (with-open-file (in file-name
                  :direction :input
                  :element-type &#39;(unsigned-byte 8))
      (let ((magic (get-term in))
            (number (get-term in))
            (row (get-term in))
            (col (get-term in)))
         (loop repeat number collect
            (make-array (* row col) 
                 :initial-contents (loop repeat (* row col) collect 
                                      (ldb (byte 8 0) (read-byte in))))))))

(defparameter *labels* (read-labels &quot;t10k-labels-idx1-ubyte&quot;))
(defparameter *train*  (read-images &quot;t10k-images-idx3-ubyte&quot;))</code></pre>
<p>I will define the topology as a toroidal grid in which the opposing
diagonal vertices are also connected.</p>
<pre><code>(defparameter ns 6)

(defun lattice (a b)
  (let* ((directions (loop for i from a to b collect i)))
    (reduce &#39;append 
            (mapcar (lambda (x) (mapcar (lambda (y) (list x y)) 
                                        directions))
                    directions))))

(defun network (a)
  (let ((directions (lattice -1 1))
        (frame (lattice 0 a)))
     (mapcar (lambda (x) (append (list x) 
                                 (mapcar (lambda (y) (mapcar (lambda (i j) (mod (+ i j) ns)) 
                                                             x y))
                                         directions)))
             frame)))

(defparameter *topology* 
     (mapcar (lambda (neigh) (mapcar (lambda (x) (+ (car x) (* ns (cadr x)))) 
                                     neigh))
             (network (1- ns))))</code></pre>
<p>The topology is explicitly given below. We have a 6x6 grid (numbered
between 0 and 35) and the first term tells us that the vertex 0 is
neighbors with 35, 5, 11, 30, 0, 6, 31, 1 and 7.</p>
<pre><code>*topology*

((0 35 5 11 30 0 6 31 1 7) (6 5 11 17 0 6 12 1 7 13)
 (12 11 17 23 6 12 18 7 13 19) (18 17 23 29 12 18 24 13 19 25)
 (24 23 29 35 18 24 30 19 25 31) (30 29 35 5 24 30 0 25 31 1)
 (1 30 0 6 31 1 7 32 2 8) (7 0 6 12 1 7 13 2 8 14)
 (13 6 12 18 7 13 19 8 14 20) (19 12 18 24 13 19 25 14 20 26)
 (25 18 24 30 19 25 31 20 26 32) (31 24 30 0 25 31 1 26 32 2)
 (2 31 1 7 32 2 8 33 3 9) (8 1 7 13 2 8 14 3 9 15)
 (14 7 13 19 8 14 20 9 15 21) (20 13 19 25 14 20 26 15 21 27)
 (26 19 25 31 20 26 32 21 27 33) (32 25 31 1 26 32 2 27 33 3)
 (3 32 2 8 33 3 9 34 4 10) (9 2 8 14 3 9 15 4 10 16)
 (15 8 14 20 9 15 21 10 16 22) (21 14 20 26 15 21 27 16 22 28)
 (27 20 26 32 21 27 33 22 28 34) (33 26 32 2 27 33 3 28 34 4)
 (4 33 3 9 34 4 10 35 5 11) (10 3 9 15 4 10 16 5 11 17)
 (16 9 15 21 10 16 22 11 17 23) (22 15 21 27 16 22 28 17 23 29)
 (28 21 27 33 22 28 34 23 29 35) (34 27 33 3 28 34 4 29 35 5)
 (5 34 4 10 35 5 11 30 0 6) (11 4 10 16 5 11 17 0 6 12)
 (17 10 16 22 11 17 23 6 12 18) (23 16 22 28 17 23 29 12 18 24)
 (29 22 28 34 23 29 35 18 24 30) (35 28 34 4 29 35 5 24 30 0))</code></pre>
<p>Here are the initial weights assigned randomly at each neuron between
0 and 255.</p>
<pre><code>(defparameter *initial-weights*
    (loop repeat (* ns ns) collect
       (make-array (* 29 29) 
                   :initial-contents
                   (loop repeat (* 29 29) collect (random 256)))))          </code></pre>
<p>And we run the SOM code using the <span
class="math inline"><em>L</em><sup>1</sup></span>-distance, a.k.a.
<em>taxi-cab</em> distance</p>
<pre><code>(defun taxi-cab (v w) 
   (/ (reduce &#39;+ (map &#39;list 
                      (lambda (i j) (abs (- i j))) 
                      v w)) 
      (length v)))

(defparameter step-1 (train-som *train* 
                                 0.015 
                                 *topology* 
                                 *initial-weights* 
                                 50 
                                 &#39;taxi-cab 
                                 1.25))</code></pre>
<p>Let me see how it worked:<br />
<img src="../media/2013-10-15-self_organizing_maps_1.png" /></p>
<p>Let me change the topology and run the code with the same initial
weights. This time, I will use an hexagonal toroidal grid.</p>
<pre><code>(defun network (a)
  (let ((directions &#39;((0 0) (0 -1) (-1 0) (1 0) (0 1) (1 1)))
        (frame (lattice 0 a)))
     (mapcar (lambda (x) (append (list x) 
                                 (mapcar (lambda (y) (mapcar (lambda (i j) (mod (+ i j) ns)) 
                                                             x y))
                                         directions)))
             frame)))

(defparameter *topology* 
     (mapcar (lambda (neigh) (mapcar (lambda (x) (+ (car x) (* ns (cadr x)))) 
                                     neigh))
             (network (1- ns))))

(defparameter step-1 (train-som *train* 
                                0.015 
                                *topology* 
                                *initial-weights* 
                                50 
                                &#39;taxi-cab 
                                1.25))</code></pre>
<p>Let us see the results:<br />
<img src="../media/2013-10-15-self_organizing_maps_2.png" /></p>
<p>And finally, on a one-dimensional circular grid but with different
set of initial weights</p>
<pre><code>(defparameter *topology* 
   (loop for i from 0 to 9 collect
       (mapcar (lambda (x) (mod (+ x i) 10)) &#39;(-1 0 1))))

(defparameter *initial-weights*
   (loop repeat 10 collect 
       (make-array (* 29 29) 
                   :initial-contents
                   (loop repeat (* 29 29) collect (random 256)))))

(defparameter step-1 (train-som *train* 
                                0.015 
                                *topology* 
                                *initial-weights* 
                                50 
                                &#39;taxi-cab 
                                1.25))       </code></pre>
<p>Let us see the results: <img
src="../media/2013-10-15-self_organizing_maps_3.png" /></p>
<h2 id="discussion">Discussion</h2>
<p>The algorithm heavily depends on the initial topology, let me call it
the <em>host</em> topology, of the neural net. However, the data
self-assembles itself depending on its own natural topology as far as
the host topology accommodates. For example, in the third example above
the data assembled itself into basically 3 separate clusters from 0’s to
1’s to 9’s then back to 0’s and some transition states in between.
However, in the first example, 1’s and 9’s clustered together (remember
<a href="http://en.wikipedia.org/wiki/Wraparound_(video_games)">the
network is toroidal</a>) as 0’s, 2’s and 6’s, also 3’s and 5’s are
always neighboring each other.</p>
<div id="footer">
<p><span id="timestamp">October 15th, 2013 11:18am</span></p>
</div>
</body>
</html>
