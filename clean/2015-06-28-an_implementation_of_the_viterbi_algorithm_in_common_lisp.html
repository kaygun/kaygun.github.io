<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2015-06-28-an_implementation_of_the_viterbi_algorithm_in_common_lisp</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="style.css" />
  <html>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <base href="https://kaygun.github.io/">
  <link rel="stylesheet" type="text/css" href="style.css">


  <head>
  <title>The Kitchen Sink and Other Oddities</title>
  </head>

  <body>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ['\\(','\\)'] ]
    }
  });
  </script>

  <div class="header">
  <h1><a href="https://kaygun.github.io">The Kitchen Sink and Other Oddities</a></h1>
  <p><a href="https://web.itu.edu.tr/kaygun">Atabey Kaygun</a></p>
  </div>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="an-implementation-of-the-viterbi-algorithm-in-common-lisp">An
implementation of the Viterbi algorithm in Common Lisp</h1>
<h2 id="the-setup">The setup</h2>
<p>Assume we a discrete sequence of observations <span
class="math inline"><em>x</em><sub>1</sub>, …, <em>x</em><sub><em>n</em></sub></span>
where each observation can be assigned a label from a finite set <span
class="math inline"><em>y</em><sub>1</sub>, …, <em>y</em><sub><em>m</em></sub></span>.
Assume also that we know the probability distribution for the labels
<span class="math inline"><em>p</em>(<em>y</em>)</span>, and the
probability of seeing an observation <span
class="math inline"><em>x</em></span> under the label <span
class="math inline"><em>y</em></span> as <span
class="math inline"><em>p</em>(<em>x</em>∥<em>y</em>)</span>. Finally,
assume that we also have transition probabilities: seeing label <span
class="math inline"><em>y</em>′</span> right after seeing a label <span
class="math inline"><em>y</em></span> as <span
class="math inline"><em>p</em>(<em>y</em>′∥<em>y</em>)</span>.</p>
<h2 id="the-problem">The problem</h2>
<p>Find an assignment of labels <span
class="math inline">ℓ: {1, …, <em>n</em>} → {1, …, <em>m</em>}</span>
such that the sequence <span
class="math display"><em>y</em><sub>ℓ(1)</sub>, …, <em>y</em><sub>ℓ(<em>n</em>)</sub></span>
is the most likely among all such sequences.</p>
<h2 id="the-theoretical-solution">The theoretical solution</h2>
<p>From <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’
Theorem</a> we see that <span class="math display">$$ p(y\|x) =
\frac{p(x\|y)p(y)}{p(x)} $$</span> This means, given a labeling scheme
<span
class="math inline">ℓ: {1, …, <em>n</em>} → {1, …, <em>m</em>}</span>,
the probability of the sequence of labels is <span
class="math display">$$
\pi(\ell) = \prod_{i=1}^n p(y_{\ell(i)}\|x_i)
p(y_{\ell(i+i)}\|y_{\ell(i)}) $$</span> After using Bayes’ Theorem we
get <span class="math display">$$ \pi(\ell) = \prod_{i=1}^n
\frac{p(x_i\|y_{\ell(i)})p(y_{\ell(i)})}{p(x_i)}
\prod_{i=1}^{n-1} p(y_{\ell(i+1)}\|y_{\ell(i)}) $$</span> Since <span
class="math inline"><em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>
does not depend on <span class="math inline">ℓ</span> we can remove it
and we get <span class="math display">$$
\pi(\ell) \sim \prod_{i=1}^n p(x_i\|y_{\ell(i)})
p(y_{\ell(i)}) p(y_{\ell(i)}\|y_{\ell(i-1)}) $$</span> Here, we use the
convention that <span
class="math inline"><em>p</em>(<em>y</em><sub>ℓ(1)</sub>∥<em>y</em><sub>ℓ(0)</sub>) = 1</span>.
Now, we need to maximize this quantity by choosing a suitable <span
class="math inline">ℓ</span>.</p>
<h2 id="recursion-with-no-memory">Recursion with no memory</h2>
<p>If we consider the problem one step at a time (which does not give
the correct answer) the problem recursively reduces to:</p>
<blockquote>
<p>Given <span class="math inline">ℓ(<em>i</em>−1)</span>, find <span
class="math inline">ℓ(<em>i</em>) ∈ {1, …, <em>m</em>}</span> such that
the quantity <span
class="math display"><em>p</em>(<em>x</em><sub><em>i</em></sub>∥<em>y</em><sub>ℓ(<em>i</em>)</sub>)<em>p</em>(<em>y</em><sub>ℓ(<em>i</em>)</sub>)<em>p</em>(<em>y</em><sub>ℓ(<em>i</em>)</sub>∥<em>y</em><sub>ℓ(<em>i</em>−1)</sub>)</span>
is maximal.</p>
</blockquote>
<p>For now, let us run with this. The recursive reduction tells us that
if we define a function <span
class="math inline"><em>a</em>(<em>k</em>,<em>j</em>,<em>i</em>)</span>
where <span
class="math display"><em>a</em>(<em>k</em>,<em>j</em>,<em>i</em>) = <em>p</em>(<em>x</em><sub><em>j</em></sub>∥<em>y</em><sub><em>k</em></sub>)<em>p</em>(<em>y</em><sub><em>k</em></sub>)<em>p</em>(<em>y</em><sub><em>k</em></sub>∥<em>y</em><sub><em>i</em></sub>)</span>
then <span
class="math display">ℓ(<em>i</em>) = argmax<sub><em>k</em></sub><em>a</em>(<em>k</em>,<em>i</em>,ℓ(<em>i</em>−1))</span></p>
<h2 id="an-implementation">An implementation</h2>
<p>I will assume that the probabilities are defined as arrays, with one
or two indices depending on whether we have ordinary probability
distribution or a conditional one. And we define</p>
<pre><code>(defun viterbi (obs init-state p-init p-tran p-emit)
   (let* ((m (array-dimension p-emit 0)))
       (labels
          ((next (j i)
              (car (sort
                      (loop for k from 0 below m collect
                         (cons k (* (aref p-init k)
                                    (aref p-emit k j)
                                    (if (&gt; i -1)
                                       (aref p-tran i k)
                                       1.0))))
                      (lambda (x y) (&gt; (cdr x) (cdr y))))))
           (iter (obs res sco)
              (if (null obs)
                  (cons (cdr (reverse res)) sco)
                  (let ((x (next (car obs) (car res))))
                     (iter (cdr obs)
                           (cons (car x) res)
                           (+ sco (- (log (cdr x)))))))))
         (iter obs &#39;(-1) 0))))
VITERBI</code></pre>
<p>In the implementation I took the negative logarithms of the
probabilities to prevent underflow.</p>
<h2 id="examples">Examples</h2>
<p>Consider the following example taken from <a
href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi Algorithm
page on Wikipedia</a>.</p>
<pre><code>(let ((p-init  #(0.6 0.4))
      (p-tran #2A((0.7 0.3) (0.4 0.6)))
      (p-emit #2A((0.1 0.4 0.5)
                  (0.6 0.3 0.1))))
  (viterbi &#39;(2 1 0) -1 p-init p-tran p-emit))
((0 0 1) . 5.618853263870896)</code></pre>
<p>The following example is a modified version of the example from <a
href="http://www.nature.com/nbt/journal/v22/n10/fig_tab/nbt1004-1315_F1.html">a
Nature paper.</a></p>
<pre><code>(let ((p-init  #(0.3 0.3 0.3))
      (p-tran #2A((0.75 0.25 0.00)
                  (0.00 0.00 1.00)
                  (0.00 0.00 1.00)))
      (p-emit #2A((0.25 0.25 0.25 0.25)
                  (0.15 0.00 0.85 0.00)
                  (0.40 0.10 0.10 0.40)))
      (seq (mapcar (lambda (x) (position x &#39;(:a :c :g :t)))
                  &#39;(:c :t :t :c
                    :a :t :g :t
                    :g :a :a :a
                    :g :c :a :g
                    :a :c :g :t
                    :a :a :g :t
                    :c :a :g :t))))
  (viterbi seq -1 p-init p-tran p-emit))
((0 0 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2) . 76.73498296015836)</code></pre>
<h2 id="what-is-next">What is next?</h2>
<p>The recursive reduction looks only at the previous step, which is not
the right way to find the solution. That is why the result I obtained
above for the genomic sequence differs from the original given in the <a
href="http://www.nature.com/nbt/journal/v22/n10/fig_tab/nbt1004-1315_F1.html">paper
I mentioned above.</a> Also, the matrix for the transition probabilities
for the labels needs to be generated for the problem at hand carefully.
One can use <a
href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm">Baum-Welch
Algorithm</a> to do just that.</p>
<div id="footer">
<p><span id="timestamp">June 28th, 2015 5:09pm</span></p>
</div>
</body>
</html>
