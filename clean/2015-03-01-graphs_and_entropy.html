<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2015-03-01-graphs_and_entropy</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="/home/kaygun/local/lib/mlisp.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="graphs-and-entropy">Graphs and Entropy</h1>
<h2 id="description-of-the-problem">Description of the problem</h2>
<p>Assume we have a directed graph with weighted edges. In other
words,we have a triple <span class="math inline">\((V,E,w)\)</span> such
that</p>
<ol type="1">
<li><span class="math inline">\(V\)</span> is a finite set</li>
<li><span class="math inline">\(E\)</span> is a set of ordered pairs
<span class="math inline">\((a,b)\)</span> of elements from <span
class="math inline">\(V\)</span></li>
<li><span class="math inline">\(w\colon E\to \mathbb{N}\)</span> is a
positive integer valued function</li>
</ol>
<p>One can assume that weights can be arbitrary real numbers, but for
thepurposes of today’s note, positive integer weights will do.</p>
<p>Today’s question is</p>
<blockquote>
<p>How much randomness does a directed graph carry and how do
wecalculate it?</p>
</blockquote>
<h2 id="theory">Theory</h2>
<p>For every graph <span class="math inline">\(G\)</span>, assume we
have such a hypothetical measure<span
class="math inline">\(\mu(G)\)</span>. We would like this measure to
satisfy</p>
<p><strong>Locality</strong>: The measure must be local, that is, one
should be able to calculateit by looking at every vertex and
<em>only</em> those vertices adjacent tothat vertex.</p>
<p><strong>Additivity</strong>: If a graph contains two disjoint
connected component, the totalmeasure should be the sum of the measures
on each component.</p>
<h2 id="some-postulates">Some Postulates</h2>
<p>I would like to say that the complete graph on <span
class="math inline">\(n\)</span> vertices in whichevery edge has the
same weight contains no information. For examplethe directed version of
<span class="math inline">\(K_3\)</span> (complete graph on 3 vertices)
depicted below should not contain any information as every vertex is
connected every other vertex in every way imaginable.</p>
<figure data-orig-height="291" data-orig-width="177" data-orig-src="../media/2015-03-01-graphs_and_entropy_0.png">
<img src="../media/2015-03-01-graphs_and_entropy_0.png" data-orig-height="291"
data-orig-width="177" data-orig-src="../media/2015-03-01-graphs_and_entropy_0.png" />
</figure>
<p>I would also like to say graphs in which there is no preference in
the in the incoming edges contain no information either. Such as the
line graphs <span class="math inline">\(A_n\)</span> on <span
class="math inline">\(n\)</span> vertices,</p>
<figure data-orig-height="291" data-orig-width="59" data-orig-src="../media/2015-03-01-graphs_and_entropy_1.png">
<img src="../media/2015-03-01-graphs_and_entropy_1.png" data-orig-height="291"
data-orig-width="59" data-orig-src="../media/2015-03-01-graphs_and_entropy_1.png" />
</figure>
<p>or cycle graphs <span class="math inline">\(C_n\)</span> on <span
class="math inline">\(n\)</span> vertices:</p>
<figure data-orig-height="291" data-orig-width="112" data-orig-src="../media/2015-03-01-graphs_and_entropy_2.png">
<img src="../media/2015-03-01-graphs_and_entropy_2.png" data-orig-height="291"
data-orig-width="112" data-orig-src="../media/2015-03-01-graphs_and_entropy_2.png" />
</figure>
<h2 id="a-candidate">A Candidate</h2>
<p>The entropy is usually good measure to measure randomness in any
probabilistic system. So, our first approximation is the randomness at a
specific vertex. I’d like to measure the randomness in the incoming
edges of a vertex. Assume <span class="math inline">\((V,E,w)\)</span>
is a weighted graph. I will write <span
class="math inline">\(w_{ab}\)</span> for the weight of an edge <span
class="math inline">\(a\to b\)</span>. I will use the convention <span
class="math display">\[ w_{\cdot b} = \sum_{a\in V} w_{ab} \quad\text{
and }\quad w_{a\cdot} = \sum_{b\in V} w_{ab} \]</span> In that case, I
would like to say that <span class="math display">\[ -\sum_{a\in V}
\frac{w_{ab}}{w_{\cdot
b}} \log_2\left(\frac{w_{ab}}{w_{\cdot b}}\right) \]</span> measures the
amount of information of the incoming edges of a vertex contains. Notice
that this measure is scale invariant: if we replace <span
class="math inline">\(w\)</span> with <span
class="math inline">\(\lambda w\)</span> we get the same measure.
However, this measure goes against our postulate that <span
class="math inline">\(\mu(K_n)\)</span> must be 0 since for every vertex
in <span class="math inline">\(K_n\)</span> we get <span
class="math inline">\(\log_2(n)\)</span>. So, we will correct this
measure by setting <span class="math display">\[ E(b) = \log_2(\deg(b))
+ \sum_{a\in V}
\frac{w_{ab}}{w_{\cdot b}} \log_2\left(\frac{w_{ab}}{w_{\cdot
b}}\right) \]</span> which can be simplified to <span
class="math display">\[ E(b) =
\log_2(\deg(b)) - \log_2(w_{\cdot b}) + \frac{1}{w_{\cdot
b}}\sum_{a\in V} w_{ab}\log_2(w_{ab}) \]</span> Then we define <span
class="math display">\[
\mu(G) = \sum_{b\in V} E(b) = \sum_{b\in V} \log_2(\deg(b)) -
\sum_{b\in V}\log_2(w_{\cdot b}) +  \sum_{a,b\in V}
\frac{w_{ab}}{w_{\cdot b}}\log_2(w_{ab}) \]</span> This measure is both
local and additive. On top of that <span
class="math inline">\(\mu(K_n)\)</span>, <span
class="math inline">\(\mu(A_n)\)</span> and <span
class="math inline">\(\mu(C_n)\)</span> are all zero.</p>
<p>The measure actually satisfies much more: if <span
class="math inline">\((a,b)\in E\)</span> is a bridge between two
components <span class="math inline">\(G_1\)</span> and <span
class="math inline">\(G_2\)</span>,</p>
<figure data-orig-height="197" data-orig-width="80" data-orig-src="../media/2015-03-01-graphs_and_entropy_3.png">
<img src="../media/2015-03-01-graphs_and_entropy_3.png" data-orig-height="197"
data-orig-width="80" data-orig-src="../media/2015-03-01-graphs_and_entropy_3.png" />
</figure>
<p>i.e. removing <span class="math inline">\((a,b)\)</span> causes <span
class="math inline">\(G\)</span> to split into 2 connected components,
then <span class="math inline">\(\mu(G) = \mu(G_1) + \mu(G_2)\)</span>
which is quite nice.</p>
<h2 id="implementation">Implementation</h2>
<p>I am going to represent each edge as a list of vertices <span
class="math inline">\((a b)\)</span> and weighted edges are going to be
a <code>assoc</code> list such as</p>
<pre><code>(defvar G
   `(((0 1) . 2)
     ((0 2) . 3)
     ((1 3) . 4)
     ((2 3) . 5)))
G</code></pre>
<p>which will represent</p>
<figure data-orig-height="291" data-orig-width="131" data-orig-src="../media/2015-03-01-graphs_and_entropy_4.png">
<img src="../media/2015-03-01-graphs_and_entropy_4.png" data-orig-height="291"
data-orig-width="131" data-orig-src="../media/2015-03-01-graphs_and_entropy_4.png" />
</figure>
<p>I have been talking about <a
href="2014-09-07-cons_is_your_friend.html"><code>map-reduce</code></a>
and this is going to be good place to put it use:</p>
<pre><code>(defun map-reduce (map red lis)
   (labels ((proc (x tail)
              (if (null x)
                 tail
                 (proc (cdr x)
                       (funcall red
                                (funcall map (car x))
                                tail)))))
       (proc lis nil)))
MAP-REDUCE</code></pre>
<p>So, <code>map-reduce</code> takes a list <code>lis</code> maps over
it via <code>map</code> and then combines the results using
<code>red</code>. The following functions calculate the total flow (in
terms of weights) and the degree of a vertex.</p>
<pre><code>(defun safe-add (x y) (+ (or x 0) (or y 0)))
SAFE-ADD

(defun flow (v G extract)
  (map-reduce (lambda (x)
                 (if (equal v (funcall extract x))
                    (cdr x)
                    0))
              #&#39;safe-add
              G))
FLOW

(defun degree (v G extract)
  (map-reduce (lambda (x)
                 (if (equal v (funcall extract x))
                    1
                    0))
              #&#39;safe-add
              G))
DEGREE</code></pre>
<p>The passed argument <code>extract</code> extracts either the source
or the target of an edge depending on whether we want the outgoing flow,
incoming flow, outgoing degree or incoming degree.</p>
<p>Here comes the entropy function:</p>
<pre><code>(defun entropy (v G extract)
  (labels ((saf (x) (if (zerop x) 1 x))
           (fn (x) (* x (log x 2))))
    (let ((deg (saf (degree v G extract)))
          (sum (saf (flow v G extract)))
          (int (map-reduce (lambda (x)
                             (if (equal v (funcall extract x))
                                 (fn (cdr x))
                                 0))
                           #&#39;safe-add
                           G)))
      (+ (log deg 2) (- (log sum 2)) (/ int sum)))))
ENTROPY</code></pre>
<p>And then we define the total entropy function as:</p>
<pre><code>(defun total-entropy (G extract)
   (let ((vertices (remove-duplicates (map-reduce #&#39;car #&#39;append G))))
      (map-reduce (lambda (x) (entropy x G extract))
                  #&#39;safe-add
                  vertices)))
TOTAL-ENTROPY</code></pre>
<h2 id="some-tests">Some Tests</h2>
<p>Let me define the following first before I do any tests:</p>
<pre><code>(defun K (n)
   (loop for i from 0 below n append
      (loop for j from 0 below n collect
         (cons (list i j) 1))))
K

(defun A (n)
   (loop for i from 0 below (1- n) collect
      (cons (list i (1+ i)) 1)))
A

(defun C (n)
   (cons (cons (list (1- n) 0) 1) (A n)))
C</code></pre>
<p>and calculate</p>
<pre><code>(total-entropy (K (1+ (random 100))) #&#39;cadar)
0.0f0
(total-entropy (A (1+ (random 100))) #&#39;cadar)
0.0f0
(total-entropy (C (1+ (random 100))) #&#39;caar)
0.0f0</code></pre>
<p>How about the graph I defined earlier?</p>
<pre><code>(total-entropy G #&#39;cadar)
0.008924007f0
(total-entropy G #&#39;caar)
0.029049516f0</code></pre>
<p>So, by looking at these numbers, I would<a
href="http://web.bahcesehir.edu.tr/atabey_kaygun/Blog/graphs-and-entropy.html"
class="uri">http://web.bahcesehir.edu.tr/atabey_kaygun/Blog/graphs-and-entropy.html</a>
say that the outgoing edges of <span class="math inline">\(G\)</span>
has more randomness than the incoming edges. And if I modify the graph
into</p>
<pre><code>(defvar H
   `(((0 1) . 1)
     ((0 2) . 2)
     ((1 3) . 1)
     ((2 3) . 2)))
H</code></pre>
<figure data-orig-height="291" data-orig-width="131" data-orig-src="../media/2015-03-01-graphs_and_entropy_5.png">
<img src="../media/2015-03-01-graphs_and_entropy_5.png" data-orig-height="291"
data-orig-width="131" data-orig-src="../media/2015-03-01-graphs_and_entropy_5.png" />
</figure>
<p>I will get the same numbers</p>
<pre><code>(total-entropy H #&#39;cadar)
0.0817042f0
(total-entropy H #&#39;caar)
0.0817042f0</code></pre>
<p>because the graph is symmetric.</p>
<h2 id="an-example">An example</h2>
<p>I had been looking at the Math PhD network. By calculating the graph
entropy I’d like to see if there was any difference between randomness
of the incoming and the outgoing edges.</p>
<p>So, first a function to load the data up. Data consists of 6 columns
separated by tabs. Columns are the id of the math PhD degree coming from
<a href="http://genealogy.math.ndsu.nodak.edu/">the Math Genealogy
Project</a>, advisor’s school (name and country) and PhD granting
school’s name and country. Data is loaded as a assoc list based on an
offset: if the offset is 0 then we construct the graph depending on the
individual school, if the offset is 1 then we do a country based
graph.</p>
<pre><code>(defun load-data (name offset)   
   (with-open-file (in name :direction :input)
      (let ((res nil)
            (pos-a (+ offset 1))
            (pos-b (+ offset 3)))
         (do ((line (read-line in nil) (read-line in nil)))
             ((null line) res)
           (let* ((local (cl-ppcre:split #\Tab line))
                  (a (elt local pos-a))
                  (b (elt local pos-b)))
              (if (assoc (list a b) res :test #&#39;equal)
              (incf (cdr (assoc (list a b) res :test #&#39;equal)))
              (push (cons (list a b) 1) res)))))))
LOAD-DATA</code></pre>
<p>Now, let us see the entropy and their differences</p>
<p>I have two data sets: one data set consists of the math PhD’s awarded
within the US, UK and Canada during the great recession (2007–2011), and
the other math PhD’s awarded within the same period within Germany,
Netherlands and Switzerland.</p>
<pre><code>(defvar data1 (load-data &quot;graphs-and-entropy-data1.csv&quot; 1))
DATA1
(total-entropy data1 #&#39;cadar)
937.71124f0
(total-entropy data1 #&#39;caar)
919.27625f0</code></pre>
<p>For the US, UK and Canadian Universities the measure of
non-randomness was larger for the incoming edges as opposed to the
outgoing edges. This means in this period, the universities show a
larger preference of hiring professors in terms of their countries than
granting PhD’s.</p>
<pre><code>(defvar data2 (load-data &quot;graphs-and-entropy-data2.csv&quot; 1))
DATA2
(total-entropy data2 #&#39;cadar)
556.59485f0
(total-entropy data2 #&#39;caar)
619.4584f0</code></pre>
<p>The situation is opposite for the universities in Germany,
Netherlands and Switzerland.</p>
<p>When we repeat the same analysis for the university based graph, we
see the same pattern. </p>
<pre><code>(defvar data3 (load-data &quot;graphs-and-entropy-data1.csv&quot; 0))
DATA3

(total-entropy data3 #&#39;cadar)
6245.294f0
(total-entropy data3 #&#39;caar)
5241.133f0

(defvar data4 (load-data &quot;graphs-and-entropy-data2.csv&quot; 0))
DATA4

(total-entropy data4 #&#39;cadar)
2096.72f0
(total-entropy data4 #&#39;caar)
2122.4478f0</code></pre>
<p>The discrepancy in the German, Netherlands and Switzerland case is
much smaller, but it is far larger in the case of the UK, US and Canada
case and in the opposite direction.</p>
<div id="footer">
<p><span id="timestamp">March 1st, 2015 4:54am</span></p>
</div>
</body>
</html>
